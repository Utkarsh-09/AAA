Vector database - Database for the AI era?

As AI continues to grow and evolve, the limitations of traditional databases become more apparent as the amount of complex, unstructured data is growing at break-neck speed. 
Enter the era of vector databases - a new breed of databases designed specifically for the demands of AI.
Lets first understand what a vector actually is.
Vectors are arrays of numbers that can represent complex objects such as words, sentences, images, or audio files in a high-dimensional space called an embedding.
These embeddings map the semantic meaning of words and similar features in other types of data. Once you have these embeddings, the challenge becomes how to store them and query them quickly.
That's where Vector databases
come in. Vector databases provide an answer to this challenge by storing arrays of numbers clustered together based on similarity, which can be queried with ultra-low latency,
 making them an ideal choice for AI-driven applications.

New money for new kind of database.
Native vector databases, such as Pinecone, Weaviate, QDrant, and Milvus, are emerging and raising millions from VCs like Andreessen Horowitz and Index Ventures.
But is all the money being thrown at these companies worth anything? Well, vectors themselves are very valuable while representing richer datasets ranging
from text, audio, images and videos, which is perfect for the myriad of use cases that generative AI has brought up. 
It isn’t that this throws traditional databases like Postgres or NoSQL database Redis out of the competition completely as postgres have tools like PG Vector to
support this type of functionality and redis also has first class Vector support but If AI is front-and-centre for most companies now, it is only natural that they will be demanding an AI-first infrastructure.

Vector databases enable a number of other use cases:
1. Search engines
2. Automated Q&A 
3. Ranking and Recommendation engines
4. Classification or near-classification solutions (e.g. anomaly detection)
5. Deep personalization
6. Low-latency edge AI applications
7. Similarity search for images, audio, video, JSON, and other forms of unstructured data
8. And, once again, generative AI tools like ChatGPT

Closing observations -
Vector databases will become essential elements in the Foundational ML/LLM stack to overcome the shortcomings of the large language models and greatly increasing their output.
ChatGPT, Langchain, Midjourney ,Stable Diffusion , etc. occupy todays discourse. And while it seems that large language models are showing a remarkable ability to mimic human reasoning 
and creative output, they are still not reliable. Even most advanced models such as GPT-4 that greatly enhanced using RLHF techniques still hallucinate, deviate or omit facts.
The use of a vector database will also facilitate tracking data lineage. This matters because if users are to trust model output, they will need to be able to track the movement of 
data over time from the source system through different forms of persistence and transformations.

Vector search and vector databases aren’t the only new systems being used for the long term memory for these LLMs
There are also certain techniques popping up recently that are able to scale these LLMs(BERT specifically) to 1M tokens and beyond with 
Recurrent Memory Transformer while maintaining high memory retrieval
accuracy and without the use of these vector databases.
 However, solving computational challenges at scale is an ongoing area of development. It will be interesting to see how these techniques develop over time.

Source :
https://gradientflow.com/the-vector-database-index/
https://gradientflow.com/vector-database-primer/
https://gradientflow.com/?s=vector+database&orderby=relevance&order=DESC
https://github.com/booydar/t5-experiments/tree/scaling-report
https://arxiv.org/pdf/2304.11062.pdf

Native vector databases, such as Pinecone, Weaviate, QDrant, and Milvus, are gaining attention and funding from prominent
 VCs due to their ability to handle vector data efficiently for similarity search, which is vital for many AI applications.
 While traditional databases like Postgres or Redis can also support vector data, they may not be as optimized as 
native vector databases. Investors are betting on the potential growth of AI, and native vector databases are seen
 as a crucial infrastructure component for building AI systems. However, as with any investment, there is always
 a risk involved.

Native vector databases, such as Pinecone, Weaviate, QDrant, and Milvus, are gaining momentum and receiving 
significant investments from VCs, as they offer optimized handling of vector data for similarity search, 
a critical requirement for many AI applications. Traditional databases like Postgres or Redis can also
 support vector data, but may not be as efficient as native vector databases. 
Investors are bullish on AI's growth potential, and native vector databases are seen as a key 
infrastructure element for building AI systems. Nonetheless, like any investment, there is always a degree of risk.

Vector databases are expected to become a crucial component of the Foundational ML/LLM stack, as they can 
address the limitations of large language models and enhance their output. 
Although advanced models such as GPT-4 have demonstrated human-like reasoning and creativity, 
they are still unreliable due to issues like hallucinations, deviations, and factual omissions. 
Vector databases can also aid in data lineage tracking, which is critical for ensuring trust in model output.
 These databases can efficiently store and retrieve high-dimensional vectors that represent complex data 
structures like text, images, and graphs.

In addition to vector search and vector databases, Recurrent Memory Transformer (RMT) is a recent technique 
that enhances the long-term memory of LLMs like BERT. RMT integrates RNNs with transformer architecture, 
allowing LLMs to handle inputs of over 1 million tokens while maintaining high accuracy in memory retrieval. 
However, scaling these techniques poses computational challenges such as large memory, computation, and 
optimization requirements, which require ongoing research for improvement.

